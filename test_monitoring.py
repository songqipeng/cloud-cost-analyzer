#!/usr/bin/env python3
"""
æµ‹è¯•ç›‘æ§æŒ‡æ ‡æ”¶é›†åŠŸèƒ½
"""
import sys
import os
import time
import threading
import json

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from cloud_cost_analyzer.monitoring import get_metrics_collector, initialize_metrics

def test_metrics_initialization():
    """æµ‹è¯•ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–"""
    print("ğŸ“Š æµ‹è¯•ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–...")
    
    config = {
        'system_collection_interval': 2,  # 2ç§’æ”¶é›†é—´éš”
        'max_history': 100
    }
    
    # åˆå§‹åŒ–ç›‘æ§ç³»ç»Ÿ
    metrics = initialize_metrics(config)
    
    print(f"   âœ… ç›‘æ§ç³»ç»Ÿå·²åˆå§‹åŒ–")
    print(f"   å¯åŠ¨çŠ¶æ€: {'å·²å¯åŠ¨' if metrics.started else 'æœªå¯åŠ¨'}")
    
    return metrics

def test_system_metrics():
    """æµ‹è¯•ç³»ç»ŸæŒ‡æ ‡æ”¶é›†"""
    print("\nğŸ’» æµ‹è¯•ç³»ç»ŸæŒ‡æ ‡æ”¶é›†...")
    
    config = {'system_collection_interval': 1}
    metrics = get_metrics_collector(config)
    metrics.start()
    
    print("   ç­‰å¾…ç³»ç»ŸæŒ‡æ ‡æ”¶é›†...")
    time.sleep(3)  # ç­‰å¾…æ”¶é›†å‡ æ¬¡æ•°æ®
    
    # è·å–æŒ‡æ ‡æ‘˜è¦
    summary = metrics.get_metrics_summary()
    
    print("   ç³»ç»ŸæŒ‡æ ‡:")
    for name, value in summary['system_metrics'].items():
        if 'percent' in name:
            print(f"     {name}: {value:.1f}%")
        elif 'mb' in name:
            print(f"     {name}: {value:.1f}MB")
        else:
            print(f"     {name}: {value:.2f}")
    
    metrics.stop()
    return True

def test_business_metrics():
    """æµ‹è¯•ä¸šåŠ¡æŒ‡æ ‡è®°å½•"""
    print("\nğŸ“ˆ æµ‹è¯•ä¸šåŠ¡æŒ‡æ ‡è®°å½•...")
    
    metrics = get_metrics_collector()
    
    # è®°å½•APIè°ƒç”¨æŒ‡æ ‡
    print("   è®°å½•APIè°ƒç”¨æŒ‡æ ‡...")
    metrics.business_collector.record_api_call(
        provider='aws',
        operation='get_cost_data',
        duration=1.5,
        success=True
    )
    
    metrics.business_collector.record_api_call(
        provider='aliyun',
        operation='get_cost_data', 
        duration=2.3,
        success=False
    )
    
    # è®°å½•æˆæœ¬åˆ†ææŒ‡æ ‡
    print("   è®°å½•æˆæœ¬åˆ†ææŒ‡æ ‡...")
    metrics.business_collector.record_cost_analysis(
        provider='aws',
        total_cost=123.45,
        service_count=5,
        analysis_duration=3.2
    )
    
    # è®°å½•ç¼“å­˜æ“ä½œæŒ‡æ ‡
    print("   è®°å½•ç¼“å­˜æ“ä½œæŒ‡æ ‡...")
    metrics.business_collector.record_cache_operation(
        operation='get',
        hit=True,
        cache_level='l1'
    )
    
    metrics.business_collector.record_cache_operation(
        operation='get',
        hit=False,
        cache_level='l2'
    )
    
    # è·å–æŒ‡æ ‡æ‘˜è¦
    summary = metrics.get_metrics_summary()
    
    print("   APIæŒ‡æ ‡:")
    for name, value in summary['api_metrics'].items():
        print(f"     {name}: {value}")
    
    print("   ç¼“å­˜æŒ‡æ ‡:")
    for name, value in summary['cache_metrics'].items():
        print(f"     {name}: {value}")
    
    return True

def test_error_metrics():
    """æµ‹è¯•é”™è¯¯æŒ‡æ ‡è®°å½•"""
    print("\nâŒ æµ‹è¯•é”™è¯¯æŒ‡æ ‡è®°å½•...")
    
    metrics = get_metrics_collector()
    
    # è®°å½•å„ç§é”™è¯¯
    print("   è®°å½•é”™è¯¯æŒ‡æ ‡...")
    errors = [
        ('ConnectionError', 'api_call', 'aws'),
        ('TimeoutError', 'api_call', 'aliyun'),
        ('ValidationError', 'data_processing', None),
        ('ConnectionError', 'api_call', 'aws'),  # é‡å¤é”™è¯¯
        ('CacheError', 'cache_operation', None)
    ]
    
    for error_type, context, provider in errors:
        metrics.error_collector.record_error(error_type, context, provider)
    
    # è·å–æŒ‡æ ‡æ‘˜è¦
    summary = metrics.get_metrics_summary()
    
    print("   é”™è¯¯æŒ‡æ ‡:")
    for name, value in summary['error_metrics'].items():
        print(f"     {name}: {value}")
    
    return True

def test_performance_timing():
    """æµ‹è¯•æ€§èƒ½è®¡æ—¶åŠŸèƒ½"""
    print("\nâ±ï¸ æµ‹è¯•æ€§èƒ½è®¡æ—¶åŠŸèƒ½...")
    
    metrics = get_metrics_collector()
    
    # ä½¿ç”¨è®¡æ—¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    print("   æµ‹è¯•æˆåŠŸæ“ä½œè®¡æ—¶...")
    with metrics.time_operation('data_processing', {'type': 'cost_analysis'}):
        time.sleep(0.5)  # æ¨¡æ‹Ÿæ“ä½œè€—æ—¶
    
    print("   æµ‹è¯•å¤±è´¥æ“ä½œè®¡æ—¶...")
    try:
        with metrics.time_operation('api_call', {'provider': 'aws', 'operation': 'get_data'}):
            time.sleep(0.2)
            raise ValueError("æ¨¡æ‹Ÿé”™è¯¯")
    except ValueError:
        pass  # é¢„æœŸçš„é”™è¯¯
    
    # æ£€æŸ¥ç”Ÿæˆçš„æŒ‡æ ‡
    current_values = metrics.registry.get_current_values()
    
    print("   æ€§èƒ½æŒ‡æ ‡:")
    for name, stats in current_values['histograms'].items():
        if 'duration' in name:
            print(f"     {name}:")
            print(f"       è°ƒç”¨æ¬¡æ•°: {stats['count']}")
            print(f"       å¹³å‡è€—æ—¶: {stats['avg']:.3f}ç§’")
            print(f"       æœ€å¤§è€—æ—¶: {stats['max']:.3f}ç§’")
    
    return True

def test_health_monitoring():
    """æµ‹è¯•å¥åº·ç›‘æ§"""
    print("\nğŸ¥ æµ‹è¯•å¥åº·ç›‘æ§...")
    
    config = {'system_collection_interval': 1}
    metrics = get_metrics_collector(config)
    metrics.start()
    
    # ç­‰å¾…æ”¶é›†ä¸€äº›ç³»ç»ŸæŒ‡æ ‡
    time.sleep(2)
    
    # è·å–å¥åº·çŠ¶æ€
    health = metrics.get_health_status()
    
    print(f"   æ•´ä½“å¥åº·çŠ¶æ€: {health['status']}")
    print("   å„é¡¹æ£€æŸ¥:")
    
    for check_name, check_result in health['checks'].items():
        status_emoji = {
            'ok': 'âœ…',
            'warning': 'âš ï¸',
            'critical': 'âŒ'
        }.get(check_result['status'], 'â“')
        
        print(f"     {status_emoji} {check_name}: {check_result['value']:.1f}")
        
        if 'threshold' in check_result:
            print(f"       é˜ˆå€¼: {check_result['threshold']}")
        elif 'thresholds' in check_result:
            print(f"       é˜ˆå€¼: {check_result['thresholds']}")
    
    print(f"   æ£€æŸ¥æ‘˜è¦:")
    summary = health['summary']
    print(f"     æ€»æ£€æŸ¥é¡¹: {summary['total_checks']}")
    print(f"     è­¦å‘Šé¡¹: {summary['warning_count']}")
    print(f"     ä¸¥é‡é¡¹: {summary['critical_count']}")
    
    metrics.stop()
    return health

def test_metrics_export():
    """æµ‹è¯•æŒ‡æ ‡å¯¼å‡º"""
    print("\nğŸ“¤ æµ‹è¯•æŒ‡æ ‡å¯¼å‡º...")
    
    metrics = get_metrics_collector()
    
    # ç”Ÿæˆä¸€äº›æŒ‡æ ‡æ•°æ®
    metrics.business_collector.record_api_call('aws', 'test_op', 1.0, True)
    metrics.error_collector.record_error('TestError', 'test_context')
    
    # å¯¼å‡ºåˆ°æ–‡ä»¶
    export_file = 'test_metrics_export.json'
    metrics.export_metrics_to_file(export_file)
    
    # æ£€æŸ¥å¯¼å‡ºæ–‡ä»¶
    if os.path.exists(export_file):
        with open(export_file, 'r') as f:
            exported_data = json.load(f)
        
        print(f"   âœ… æŒ‡æ ‡å·²å¯¼å‡ºåˆ° {export_file}")
        print(f"   å¯¼å‡ºæ—¶é—´: {exported_data['export_timestamp']}")
        print(f"   æŒ‡æ ‡ç±»å‹æ•°é‡:")
        for metric_type, count in exported_data['summary']['metrics_count'].items():
            print(f"     {metric_type}: {count}")
        
        # æ¸…ç†å¯¼å‡ºæ–‡ä»¶
        os.remove(export_file)
        
        return True
    else:
        print("   âŒ å¯¼å‡ºæ–‡ä»¶æœªç”Ÿæˆ")
        return False

def test_concurrent_metrics():
    """æµ‹è¯•å¹¶å‘æŒ‡æ ‡è®°å½•"""
    print("\nğŸ”„ æµ‹è¯•å¹¶å‘æŒ‡æ ‡è®°å½•...")
    
    metrics = get_metrics_collector()
    
    def worker(worker_id):
        """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
        for i in range(10):
            # è®°å½•APIè°ƒç”¨
            metrics.business_collector.record_api_call(
                provider=f'provider_{worker_id}',
                operation='concurrent_test',
                duration=0.1,
                success=True
            )
            
            # è®°å½•é”™è¯¯
            if i % 3 == 0:  # æ¯3æ¬¡è®°å½•ä¸€ä¸ªé”™è¯¯
                metrics.error_collector.record_error(
                    error_type='ConcurrentTestError',
                    context=f'worker_{worker_id}'
                )
            
            time.sleep(0.01)  # çŸ­æš‚å»¶è¿Ÿ
    
    # å¯åŠ¨å¤šä¸ªå·¥ä½œçº¿ç¨‹
    threads = []
    worker_count = 5
    
    print(f"   å¯åŠ¨ {worker_count} ä¸ªå¹¶å‘å·¥ä½œçº¿ç¨‹...")
    
    for i in range(worker_count):
        thread = threading.Thread(target=worker, args=(i,))
        threads.append(thread)
        thread.start()
    
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for thread in threads:
        thread.join()
    
    # æ£€æŸ¥ç»“æœ
    summary = metrics.get_metrics_summary()
    
    total_api_calls = sum(summary['api_metrics'].values())
    total_errors = sum(summary['error_metrics'].values())
    
    print(f"   âœ… å¹¶å‘æµ‹è¯•å®Œæˆ")
    print(f"   æ€»APIè°ƒç”¨: {total_api_calls}")
    print(f"   æ€»é”™è¯¯æ•°: {total_errors}")
    print(f"   é¢„æœŸAPIè°ƒç”¨: {worker_count * 10}")
    print(f"   é¢„æœŸé”™è¯¯æ•°: {worker_count * 4}")  # æ¯10æ¬¡è°ƒç”¨æœ‰çº¦4ä¸ªé”™è¯¯
    
    return True

def test_metrics_performance():
    """æµ‹è¯•æŒ‡æ ‡æ€§èƒ½"""
    print("\nâš¡ æµ‹è¯•æŒ‡æ ‡æ”¶é›†æ€§èƒ½...")
    
    metrics = get_metrics_collector()
    
    # æµ‹è¯•æŒ‡æ ‡è®°å½•æ€§èƒ½
    iterations = 1000
    
    # APIè°ƒç”¨æŒ‡æ ‡æ€§èƒ½æµ‹è¯•
    start_time = time.time()
    
    for i in range(iterations):
        metrics.business_collector.record_api_call(
            provider='perf_test',
            operation='test_op',
            duration=0.1,
            success=True
        )
    
    api_duration = time.time() - start_time
    
    # é”™è¯¯æŒ‡æ ‡æ€§èƒ½æµ‹è¯•
    start_time = time.time()
    
    for i in range(iterations):
        metrics.error_collector.record_error(
            error_type='PerfTestError',
            context='performance_test'
        )
    
    error_duration = time.time() - start_time
    
    print(f"   APIæŒ‡æ ‡è®°å½•æ€§èƒ½:")
    print(f"     {iterations} æ¬¡è®°å½•è€—æ—¶: {api_duration:.4f}ç§’")
    print(f"     å¹³å‡è€—æ—¶: {api_duration/iterations*1000:.2f}æ¯«ç§’/æ¬¡")
    print(f"     å¤„ç†é€Ÿåº¦: {iterations/api_duration:.0f}æ¬¡/ç§’")
    
    print(f"   é”™è¯¯æŒ‡æ ‡è®°å½•æ€§èƒ½:")
    print(f"     {iterations} æ¬¡è®°å½•è€—æ—¶: {error_duration:.4f}ç§’")
    print(f"     å¹³å‡è€—æ—¶: {error_duration/iterations*1000:.2f}æ¯«ç§’/æ¬¡")
    print(f"     å¤„ç†é€Ÿåº¦: {iterations/error_duration:.0f}æ¬¡/ç§’")
    
    return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ“Š ç›‘æ§æŒ‡æ ‡æ”¶é›†åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    try:
        # æ‰§è¡Œå„é¡¹æµ‹è¯•
        metrics = test_metrics_initialization()
        test_system_metrics()
        test_business_metrics()
        test_error_metrics()
        test_performance_timing()
        health = test_health_monitoring()
        test_metrics_export()
        test_concurrent_metrics()
        test_metrics_performance()
        
        print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
        print("   âœ… ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–æ­£å¸¸")
        print("   âœ… ç³»ç»ŸæŒ‡æ ‡æ”¶é›†æ­£å¸¸")
        print("   âœ… ä¸šåŠ¡æŒ‡æ ‡è®°å½•æ­£å¸¸")
        print("   âœ… é”™è¯¯æŒ‡æ ‡è®°å½•æ­£å¸¸")
        print("   âœ… æ€§èƒ½è®¡æ—¶åŠŸèƒ½æ­£å¸¸")
        print("   âœ… å¥åº·ç›‘æ§åŠŸèƒ½æ­£å¸¸")
        print("   âœ… æŒ‡æ ‡å¯¼å‡ºåŠŸèƒ½æ­£å¸¸")
        print("   âœ… å¹¶å‘æŒ‡æ ‡è®°å½•æ­£å¸¸")
        print("   âœ… æŒ‡æ ‡æ€§èƒ½æµ‹è¯•é€šè¿‡")
        print(f"   ç³»ç»Ÿæ•´ä½“å¥åº·çŠ¶æ€: {health['status']}")
        
        print("\nâœ… ç›‘æ§æŒ‡æ ‡æ”¶é›†æµ‹è¯•å®Œæˆ!")
        
        # æœ€ç»ˆæ¸…ç†
        metrics.stop()
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()