#!/usr/bin/env python3
"""
ä¼˜åŒ–åŠŸèƒ½ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ–°çš„ç¼“å­˜ã€å¼‚æ­¥ã€ç›‘æ§ã€æ—¥å¿—ç­‰åŠŸèƒ½
"""
import asyncio
import json
from datetime import datetime, timedelta

from cloud_cost_analyzer.core.enhanced_async_analyzer import (
    EnhancedAsyncMultiCloudAnalyzer, 
    analyze_multi_cloud_async
)
from cloud_cost_analyzer.cache.tiered_cache import initialize_cache, get_tiered_cache
from cloud_cost_analyzer.utils.secure_logger import get_secure_logger, mask_sensitive_data
from cloud_cost_analyzer.utils.retry import retry_manager
from cloud_cost_analyzer.monitoring import initialize_metrics, get_metrics_collector


async def main():
    """ä¸»å‡½æ•°æ¼”ç¤ºå„ç§ä¼˜åŒ–åŠŸèƒ½"""
    
    # 1. åˆå§‹åŒ–å®‰å…¨æ—¥å¿—ç³»ç»Ÿ
    logger = get_secure_logger('optimized_example')
    logger.info("Starting optimized cloud cost analysis example")
    
    # 2. åŠ è½½é…ç½®
    with open('config.optimized.json', 'r') as f:
        config = json.load(f)
    
    # 3. åˆå§‹åŒ–ç¼“å­˜ç³»ç»Ÿ
    cache = initialize_cache(config)
    logger.info("Tiered cache system initialized")
    
    # 4. åˆå§‹åŒ–ç›‘æ§ç³»ç»Ÿ
    metrics = initialize_metrics(config.get('monitoring', {}))
    logger.info("Metrics collection started")
    
    # 5. æ¼”ç¤ºæ•æ„Ÿæ•°æ®è„±æ•
    sensitive_data = {
        'aws_access_key': 'AKIAIOSFODNN7EXAMPLE',
        'aws_secret_key': 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',
        'user_info': {
            'email': 'user@example.com',
            'phone': '13812345678'
        }
    }
    
    masked_data = mask_sensitive_data(sensitive_data)
    logger.info(f"Masked sensitive data: {masked_data}")
    
    # 6. æ¼”ç¤ºå¼‚æ­¥å¤šäº‘åˆ†æ
    logger.info("Starting async multi-cloud analysis")
    
    try:
        # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºæ­£ç¡®æ¸…ç†
        async with EnhancedAsyncMultiCloudAnalyzer(config=config) as analyzer:
            
            # è®°å½•åˆ†ææ“ä½œæŒ‡æ ‡
            with metrics.time_operation('multi_cloud_analysis', {'method': 'async'}):
                result = await analyzer.analyze_multi_cloud_async(
                    providers=['aws', 'aliyun', 'tencent'],
                    start_date='2024-01-01',
                    end_date='2024-01-31'
                )
            
            logger.info("Multi-cloud analysis completed successfully")
            
            # è®°å½•ä¸šåŠ¡æŒ‡æ ‡
            metrics.business_collector.record_cost_analysis(
                provider='multi',
                total_cost=result['summary']['total_cost'],
                service_count=result['summary']['provider_count'],
                analysis_duration=result['summary']['analysis_duration']
            )
            
            # 7. æ¼”ç¤ºç¼“å­˜ç»Ÿè®¡
            cache_stats = cache.get_stats()
            logger.info(f"Cache statistics: {cache_stats}")
            
            # 8. æ¼”ç¤ºè¿æ¥çŠ¶æ€æµ‹è¯•
            logger.info("Testing all provider connections")
            connections = await analyzer.test_all_connections_async()
            for provider, (status, message) in connections.items():
                logger.info(f"{provider}: {'âœ…' if status else 'âŒ'} {message}")
                
                # è®°å½•è¿æ¥æµ‹è¯•æŒ‡æ ‡
                metrics.business_collector.record_api_call(
                    provider=provider,
                    operation='connection_test',
                    duration=0.2,  # æ¨¡æ‹Ÿè€—æ—¶
                    success=status
                )
    
    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        metrics.error_collector.record_error(
            error_type=type(e).__name__,
            context='multi_cloud_analysis'
        )
    
    # 9. æ¼”ç¤ºé‡è¯•æœºåˆ¶
    logger.info("Demonstrating retry mechanism")
    
    @retry_manager.retry_with_strategy('cloud_api')
    def unreliable_operation():
        import random
        if random.random() < 0.7:  # 70% å¤±è´¥ç‡
            raise ConnectionError("Simulated connection failure")
        return "Operation successful"
    
    try:
        result = unreliable_operation()
        logger.info(f"Retry operation result: {result}")
    except Exception as e:
        logger.error(f"Retry operation failed: {e}")
    
    # 10. æ¼”ç¤ºæ‰¹é‡åˆ†æ
    logger.info("Demonstrating batch analysis")
    
    batch_requests = [
        {
            'providers': ['aws'],
            'start_date': '2024-01-01',
            'end_date': '2024-01-15'
        },
        {
            'providers': ['aliyun', 'tencent'],
            'start_date': '2024-01-16',
            'end_date': '2024-01-31'
        }
    ]
    
    # ä½¿ç”¨ä¾¿æ·å‡½æ•°è¿›è¡Œæ‰¹é‡åˆ†æ
    from cloud_cost_analyzer.core.enhanced_async_analyzer import batch_analyze_async
    
    try:
        batch_results = await batch_analyze_async(batch_requests, config)
        logger.info(f"Batch analysis completed: {len(batch_results)} results")
    except Exception as e:
        logger.error(f"Batch analysis failed: {e}")
    
    # 11. å±•ç¤ºç›‘æ§å’Œå¥åº·çŠ¶æ€
    logger.info("Getting system health status")
    health = metrics.get_health_status()
    logger.info(f"System health: {health['status']}")
    
    for check_name, check_result in health['checks'].items():
        status_emoji = 'âœ…' if check_result['status'] == 'ok' else 'âš ï¸' if check_result['status'] == 'warning' else 'âŒ'
        logger.info(f"{check_name}: {status_emoji} {check_result['value']}")
    
    # 12. å¯¼å‡ºæŒ‡æ ‡
    logger.info("Exporting metrics to file")
    metrics.export_metrics_to_file('metrics_export.json')
    
    # 13. å±•ç¤ºæœ€ç»ˆç»Ÿè®¡
    final_stats = {
        'cache_stats': cache.get_stats(),
        'metrics_summary': metrics.get_metrics_summary(),
        'health_status': health
    }
    
    logger.info("=== Final Statistics ===")
    logger.info(f"Cache hit rate: {final_stats['cache_stats']['hit_rate']:.2%}")
    logger.info(f"Total metrics collected: {sum(final_stats['metrics_summary']['metrics_count'].values())}")
    logger.info(f"Overall system health: {final_stats['health_status']['status']}")
    
    # 14. æ¸…ç†èµ„æº
    logger.info("Cleaning up resources")
    metrics.stop()
    logger.info("Optimization example completed successfully")


# åŒæ­¥ç‰ˆæœ¬ç¤ºä¾‹
def sync_example():
    """åŒæ­¥ç‰ˆæœ¬çš„ä¼˜åŒ–åŠŸèƒ½ç¤ºä¾‹"""
    logger = get_secure_logger('sync_example')
    
    # ä½¿ç”¨ç†”æ–­å™¨ä¿æŠ¤
    from cloud_cost_analyzer.utils.retry import CircuitBreaker
    
    circuit_breaker = CircuitBreaker(failure_threshold=3, timeout=60)
    
    for i in range(5):
        try:
            with circuit_breaker:
                logger.info(f"Attempt {i + 1}")
                # æ¨¡æ‹Ÿå¯èƒ½å¤±è´¥çš„æ“ä½œ
                import random
                if random.random() < 0.8:  # 80% å¤±è´¥ç‡
                    raise ConnectionError(f"Connection failed on attempt {i + 1}")
                logger.info("Operation succeeded")
                break
        except Exception as e:
            logger.warning(f"Attempt {i + 1} failed: {e}")


if __name__ == '__main__':
    print("ğŸš€ Cloud Cost Analyzer Optimization Demo")
    print("=" * 50)
    
    # è¿è¡Œå¼‚æ­¥ç¤ºä¾‹
    print("\nğŸ“Š Running async optimization example...")
    asyncio.run(main())
    
    print("\nğŸ”„ Running sync optimization example...")
    sync_example()
    
    print("\nâœ… All examples completed!")
    print("\nFiles generated:")
    print("- metrics_export.json (ç›‘æ§æŒ‡æ ‡å¯¼å‡º)")
    print("- logs/ (æ—¥å¿—æ–‡ä»¶)")
    print("- .cache/ (ç¼“å­˜æ–‡ä»¶)")